{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CELL\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "# from jupyterthemes import jtplot\n",
    "from string import digits\n",
    "import glob\n",
    "import errno \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpos = []\n",
    "trainneg = []\n",
    "testpos = []\n",
    "testneg = []\n",
    "\n",
    "stop_words = []\n",
    "with open('./Dataset/stop_words.txt',encoding='ISO-8859-1') as f:\n",
    "    stop_words = [line.rstrip() for line in f]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['./Dataset/train/pos/*.txt', './Dataset/train/neg/*.txt', './Dataset/test/pos/*.txt', './Dataset/test/neg/*.txt']\n",
    "sent = []\n",
    "labels = ['pos','neg','pos','neg']\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "    files = glob.glob(path) \n",
    "    l = []\n",
    "    for name in files: \n",
    "        try: \n",
    "            \n",
    "            with open(name,encoding='ISO-8859-1') as f: \n",
    "                p = f.read()\n",
    "               \n",
    "                l.append(p)\n",
    "            \n",
    "        except IOError as exc: \n",
    "            print(exc)\n",
    "            if exc.errno != errno.EISDIR: \n",
    "                raise \n",
    "    sent.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmpunc(para):\n",
    "    return para.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning\n",
    "\n",
    "for i, sen in enumerate(sent):\n",
    "    Revs = pd.DataFrame(sen, columns =['Reviews'])\n",
    "    \n",
    "    #To Lowercase\n",
    "    Revs['Reviews'] = Revs['Reviews'].str.lower()\n",
    "\n",
    "    #Removing stopwords\n",
    "    Revs['Reviews'] = Revs['Reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    #Removing Punctuation\n",
    "    Revs['Reviews'] = Revs.apply(lambda row: rmpunc(row[\"Reviews\"]),axis=1 )\n",
    "    \n",
    "    sent[i] = Revs['Reviews'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpos = sent[0]\n",
    "poses = ['pos'] * len(trainpos)\n",
    "\n",
    "trainneg = sent[1]\n",
    "negs = ['neg'] * len(trainneg)\n",
    "\n",
    "\n",
    "trainrevs = trainpos + trainneg\n",
    "Train_Y = poses + negs\n",
    "\n",
    "\n",
    "testpos = sent[2]\n",
    "poses = ['pos'] * len(testpos)\n",
    "\n",
    "testneg = sent[3]\n",
    "negs = ['neg'] * len(testneg)\n",
    "\n",
    "\n",
    "testrevs = testpos + testneg\n",
    "Test_Y = poses + negs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Vocab\n",
    "all_text = ' '.join(trainrevs + testrevs)\n",
    "\n",
    "vocab = list(set(all_text.split()))\n",
    "# display(len(vocab))\n",
    "# display(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTrain = {\n",
    "    'pos': Counter((' '.join(trainpos)).split()),\n",
    "    'neg': Counter((' '.join(trainneg)).split())\n",
    "    \n",
    "}\n",
    "\n",
    "dictTest = {\n",
    "    'pos': Counter((' '.join(testpos)).split()),\n",
    "    'neg': Counter((' '.join(testneg)).split())\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaltrainrevs = len(trainrevs)\n",
    "classes = ['pos', 'neg']\n",
    "totalclassreviews = {\n",
    "    'pos': len(trainpos), 'neg':len(trainneg)\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(dictTrain, totaltrainrevs, totalclassreviews, vocab):\n",
    "    logprior = {}\n",
    "    loglikelihood = {}\n",
    "    for c in classes:\n",
    "        logprior[c] = math.log(totalclassreviews[c] / totaltrainrevs)\n",
    "        loglikelihood[c] = {}\n",
    "   \n",
    "    for word in vocab:\n",
    "        for c in dictTrain: \n",
    "            loglikelihood[c][word] = math.log((dictTrain[c][word] + 1) / ( sum(dictTrain[c].values()) + len(vocab)) )\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return logprior, loglikelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(dictTrain, totaltrainrevs, totalclassreviews, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmap(Review, vocab, logprior, loglikelihood, classes):\n",
    "    rev_words = Review.split()\n",
    "    Sum = {}\n",
    "\n",
    "\n",
    "    for c in classes:\n",
    "        Sum[c] = logprior[c]\n",
    "        result = {k: loglikelihood[c][k] for k in (loglikelihood[c].keys() & rev_words)}\n",
    "        Sum[c]+= sum(result.values())\n",
    "            \n",
    "    argmax = max(Sum, key=Sum.get) \n",
    "    \n",
    "    return argmax\n",
    "    \n",
    "\n",
    "\n",
    "def test_naive_bayes(testrevs, logprior, loglikelihood, vocab, classes):\n",
    "    testrevs = np.array(testrevs) \n",
    "\n",
    "    \n",
    "    predicted_labels = []\n",
    "    for Review in np.nditer(testrevs):\n",
    "        \n",
    "        predicted_labels.append(hmap(str(Review), vocab, logprior,loglikelihood, classes))\n",
    "\n",
    "    \n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "def match(col1, col2):\n",
    "    if col1 == col2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_accuracy(predicted_labels, actual_labels):\n",
    "    prediction = pd.DataFrame({'Predicted Sentiment':predicted_labels, 'Actual Sentiment': actual_labels })\n",
    "    \n",
    " \n",
    "    \n",
    "    prediction[\"Count\"] = prediction.apply(lambda x: match(x['Predicted Sentiment'], x['Actual Sentiment']), axis=1)\n",
    "\n",
    "\n",
    "    matches = prediction[\"Count\"].sum()\n",
    "    samples = prediction.shape[0]\n",
    "    \n",
    "    accuracy = matches / samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = test_naive_bayes(testrevs,logprior, loglikelihood, vocab, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83504"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy = get_accuracy(predicted_labels,Test_Y)\n",
    "display(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "Train_X = (vectorizer.fit_transform(trainrevs))\n",
    "Test_X = (vectorizer.fit_transform(testrevs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(Train_X, Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = clf.predict(Test_X)\n",
    "actual_labels = Test_Y\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82376"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[11016,  1484],\n",
       "       [ 2922,  9578]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix and accuracy\n",
    "display(accuracy)\n",
    "display(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
